{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa52af-35d9-486b-a46d-0f3318b1cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.private_preview import llm_extension\n",
    "\n",
    "aiplatform.init(project=\"koverholt-devrel-355716\", location=\"us-central1\")\n",
    "\n",
    "extension_bigquery = llm_extension.Extension.create(\n",
    "    display_name = \"BigQuery\",\n",
    "    description = \"Interact with the BigQuery API\",\n",
    "    manifest = {\n",
    "        \"name\": \"bigquery_tool\",\n",
    "        \"description\": \"BigQUery Extension\",\n",
    "        \"api_spec\": {\n",
    "            \"open_api_gcs_uri\": \"gs://vertex-ai-extensions-koverholt/bigquery/extension.yaml\"\n",
    "        },\n",
    "        \"auth_config\": {\n",
    "            \"google_service_account_config\": {},\n",
    "            \"auth_type\": \"GOOGLE_SERVICE_ACCOUNT_AUTH\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(extension_bigquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae97432-4031-44ae-97bc-caa8a4cb494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.private_preview import llm_extension\n",
    "\n",
    "aiplatform.init(project=\"koverholt-devrel-355716\", location=\"us-central1\")\n",
    "\n",
    "TOOL_SELECTION_PROMPT = \"\"\" You need to select one of the tools which can resolve this user query.\n",
    "You should output the tool_name that you select.\n",
    "\n",
    "TOOLs: {tool_descriptions}\n",
    "\n",
    "USER QUERY: {query}\n",
    "SELECTED tool_name:\n",
    "\"\"\"\n",
    "\n",
    "TOOL_INVOCATION_PROMPT = \"\"\" Given a user query and a tool, you need to predict input parameters as a JSON to trigger this tool which can answer this user query.\n",
    "\n",
    "TOOL INPUT FORMAT:\n",
    "{input_params}\n",
    "\n",
    "{invocation_examples}\n",
    "USER QUERY: {query}\n",
    "RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "RESPONSE_PROMPT = \"\"\"You should understand the format of this json OUTPUT by the following TOOL OUTPUT FORMAT,\n",
    "and then use this json OUTPUT, find relevant information, summarize, answer the user query and reply in RESPONSE.\n",
    "Your RESPONSE must answer the user query. If you don't find relevant information from OUTPUT, just reply \"Sorry I don't know\".\n",
    "Your RESPONSE must be related to the OUTPUT.\n",
    "If the user query is related to code and the output contains code, your response should also contain the code snippets.\n",
    "\n",
    "Your RESPONSE should be informative, and simple for users to follow and understand.\n",
    "\n",
    "TOOL OUTPUT FORMAT:\n",
    "{output_params}\n",
    "\n",
    "OUTPUT:\n",
    "{output}\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class SingleActionAgent:\n",
    "    def __init__(self):\n",
    "        self.TOOL_SELECTION_PROMPT = TOOL_SELECTION_PROMPT\n",
    "        self.TOOL_INVOCATION_PROMPT = TOOL_INVOCATION_PROMPT\n",
    "        self.RESPONSE_PROMPT = RESPONSE_PROMPT\n",
    "        self.llm_model_name = \"text-bison@001\"\n",
    "        self.llm_max_output_tokens = 512\n",
    "        self.llm_temperature = 0.1\n",
    "        self.llm_top_p = 0.8\n",
    "        self.llm_top_k = 40\n",
    "        self.project_id = 'koverholt-devrel-355716'\n",
    "        self.location = 'us-central1'\n",
    "        self.staging_bucket = f'koverholt-dev-extensions'\n",
    "        self.tool_infos = [\n",
    "            {\n",
    "                \"extension_id\": \"4091098049001029632\", # exchange rate tool\n",
    "                \"operation_id\": \"get_exchange_rate\",\n",
    "                \"output_type\": \"Result\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def set_up(self):\n",
    "        from langchain import llms\n",
    "        from google.cloud import aiplatform\n",
    "        from google.cloud.aiplatform.private_preview import llm_extension\n",
    "\n",
    "        aiplatform.init(\n",
    "            project=self.project_id,\n",
    "            location=self.location,\n",
    "            staging_bucket=self.staging_bucket,\n",
    "        )\n",
    "\n",
    "        self.llm = llms.VertexAI(\n",
    "            model_name=self.llm_model_name,\n",
    "            max_output_tokens=self.llm_max_output_tokens,\n",
    "            temperature=self.llm_temperature,\n",
    "            top_p=self.llm_top_p,\n",
    "            top_k=self.llm_top_k,\n",
    "        )\n",
    "        self.tool_infos = [\n",
    "            self._generate_tool_info(\n",
    "                extension=llm_extension.Extension(\n",
    "                    f\"projects/{self.project_id}/locations/{self.location}/extensions/{tool_info['extension_id']}\",\n",
    "                    # api_base_path_override='autopush-aiplatform.sandbox.googleapis.com',\n",
    "                ),\n",
    "                operation_id=tool_info[\"operation_id\"],\n",
    "                output_type=tool_info[\"output_type\"],\n",
    "            )\n",
    "            for tool_info in self.tool_infos\n",
    "        ]\n",
    "\n",
    "    def _get_tool_description_string(self):\n",
    "        result_string = \"\"\n",
    "        for i, tool_info in enumerate(self.tool_infos):\n",
    "            result_string += f\"\"\"\n",
    "            {i+1}. tool_name: {tool_info['name']}\n",
    "              tool_description:\n",
    "                  {tool_info['operation_description']}\n",
    "                  {tool_info['extension_description']}\n",
    "            \"\"\"\n",
    "        return result_string\n",
    "\n",
    "    def _get_tool_info_with_name(self, name):\n",
    "        for tool_info in self.tool_infos:\n",
    "            if tool_info[\"name\"] == name:\n",
    "                return tool_info\n",
    "        raise Exception(f\"Tool {name} is not supported.\")\n",
    "\n",
    "    def _run_vertex_extension(\n",
    "            self,\n",
    "            extension,\n",
    "            operation_id: str,\n",
    "            operation_params: dict\n",
    "        ) -> dict:\n",
    "        response = extension.execute(operation_id, operation_params)\n",
    "        logging.info(f\"\\n Vertex Extension Response: {response}\")\n",
    "        return response\n",
    "\n",
    "    def _llm_select_tool(self, query):\n",
    "        tool_descriptions = self._get_tool_description_string()\n",
    "        tool_name = self.llm.predict(self.TOOL_SELECTION_PROMPT.format(\n",
    "            tool_descriptions=tool_descriptions,\n",
    "            query=query,\n",
    "        ))\n",
    "        logging.info(\"\\n Selected Tool: \" + tool_name)\n",
    "        return self._get_tool_info_with_name(tool_name)\n",
    "\n",
    "    def _llm_predict_params(self, query, selected_tool_info):\n",
    "        input_params = selected_tool_info['input_params']\n",
    "        invocation_examples = selected_tool_info['invocation_examples'] if \"invocation_examples\" in selected_tool_info else \"\"\n",
    "        params_string = self.llm.predict(\n",
    "            self.TOOL_INVOCATION_PROMPT.format(\n",
    "                input_params=input_params,\n",
    "                invocation_examples=invocation_examples,\n",
    "                query=query))\n",
    "        logging.info(\"\\n Predicted Params String: \" + params_string)\n",
    "        params_json = json.loads(params_string)\n",
    "        logging.info(\"\\n Parsed Tool Params: \" + str(params_json))\n",
    "        return params_json\n",
    "\n",
    "    def _llm_generate_response(self, query, api_output, selected_tool_info):\n",
    "        response = self.llm.predict(self.RESPONSE_PROMPT.format(\n",
    "            query=query,\n",
    "            output=api_output,\n",
    "            output_params=selected_tool_info['output_params'],\n",
    "        ))\n",
    "        logging.info(\"\\n Response: \" + response)\n",
    "        return response\n",
    "\n",
    "    def _find_operation_id(self, json_data, operation_id):\n",
    "        \"\"\"Recursively searches for the dictionary with the specified operationId value.\"\"\"\n",
    "        if (\"operationId\" in json_data and json_data[\"operationId\"] == operation_id):\n",
    "            return json_data\n",
    "        for key, value in json_data.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Recursively search within nested dictionaries\n",
    "                found_dict = self._find_operation_id(value, operation_id)\n",
    "                if found_dict:\n",
    "                    return found_dict\n",
    "        return None  # No matching dictionary found\n",
    "\n",
    "    def _generate_tool_info(\n",
    "            self,\n",
    "            extension,\n",
    "            operation_id: str,\n",
    "            output_type: str,\n",
    "        ):\n",
    "        open_api_struct = extension.api_spec()\n",
    "        operation_struct = self._find_operation_id(open_api_struct, operation_id)\n",
    "        output_params = open_api_struct\n",
    "        return {\"extension\": extension,\n",
    "                \"operation_id\": operation_id,\n",
    "                \"name\": f\"{open_api_struct['info']['title']}/{operation_id}\",\n",
    "                \"extension_description\": open_api_struct['info']['description'],\n",
    "                \"operation_description\": operation_struct['description'],\n",
    "                \"input_params\": operation_struct['parameters'],\n",
    "                \"output_params\": output_params}\n",
    "\n",
    "    def query(self, query):\n",
    "        selected_tool_info = self._llm_select_tool(query)\n",
    "        params_json = self._llm_predict_params(query, selected_tool_info)\n",
    "        print(\"API Parameters\" + str(params_json))\n",
    "        api_output = self._run_vertex_extension(\n",
    "            selected_tool_info['extension'],\n",
    "            selected_tool_info['operation_id'],\n",
    "            params_json,\n",
    "        )\n",
    "        print(\"API Response: \" + str(api_output))\n",
    "        response = self._llm_generate_response(query, api_output, selected_tool_info)\n",
    "        return response\n",
    "\n",
    "agent = SingleActionAgent()\n",
    "\n",
    "agent.set_up()\n",
    "\n",
    "print()\n",
    "query = \"What is the exchange rate from USD to EUR\"\n",
    "print(\"Query: \" + query)\n",
    "print(\"LLM + Extensions Response: \" + agent.query(query))\n",
    "\n",
    "print()\n",
    "query = \"What was the exchange rate of Canadian dollars to Swedish Krona as of Jan 31, 2022?\"\n",
    "print(\"Query: \" + query)\n",
    "print(\"LLM + Extensions Response: \" + agent.query(query))\n",
    "\n",
    "print()\n",
    "query = \"How much would $1000 in Australia be worth in Japanese yen as of mid 2020?\"\n",
    "print(\"Query: \" + query)\n",
    "print(\"LLM + Extensions Response: \" + agent.query(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
